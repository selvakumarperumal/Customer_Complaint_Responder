{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "764f3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Sequence, Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f6c0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_message(self, message: BaseMessage) -> None:\n",
    "        self.messages.append(message)\n",
    "\n",
    "    def add_messages(self, messages: Sequence[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages.clear()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00c22abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human: Hello!\n",
      "ai: Hi, how can I help you?\n",
      "human: I have a complaint.\n",
      "ai: Please describe your issue.\n"
     ]
    }
   ],
   "source": [
    "# Create an InMemoryHistory object and add messages\n",
    "history = InMemoryHistory()\n",
    "history.add_messages([HumanMessage(content=\"Hello!\"), AIMessage(content=\"Hi, how can I help you?\")])\n",
    "history.add_messages((HumanMessage(content=\"I have a complaint.\"), AIMessage(content=\"Please describe your issue.\")))\n",
    "\n",
    "# Print the message history\n",
    "for msg in history.messages:\n",
    "    print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cf960bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_histories: Dict[str, BaseChatMessageHistory] = {}\n",
    "\n",
    "def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"Retrieve or Create chat message history by session ID.\"\"\"\n",
    "    if session_id not in message_histories:\n",
    "        message_histories[session_id] = InMemoryHistory()\n",
    "    return message_histories[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f59247bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19662a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(\n",
    "    api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    model_name=\"mistral-small-latest\",\n",
    "    max_retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98645cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful customer support assistant for an e-commerce platform.\"),\n",
    "    (\"user\", \"Below there is a complaint. Respond to that in a polite and helpful manner.\"),\n",
    "    (\"user\", \"Complaint: {complaint}\"),\n",
    "    (\"user\", \"Message History:\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\")\n",
    "])  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8864bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful customer support assistant for an e-commerce platform.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Below there is a complaint. Respond to that in a polite and helpful manner.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Complaint: I received the wrong item.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Message History:', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I want to return this item.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you help me with the return process?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.format_messages(\n",
    "    complaint=\"I received the wrong item.\",\n",
    "    history=[\n",
    "        HumanMessage(content=\"I want to return this item.\"),\n",
    "        HumanMessage(content=\"Can you help me with the return process?\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LCEL: Chain the prompt and LLM using | operator (LangChain Expression Language)\n",
    "# Chain prompt and LLM\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80255f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_complaint(input: dict) -> dict:\n",
    "    session_id = input[\"session_id\"]\n",
    "    complaint = input[\"complaint\"]\n",
    "    history = get_by_session_id(session_id)\n",
    "    response = prompt.format_messages({\n",
    "        \"complaint\": complaint,\n",
    "        \"history\": history.messages\n",
    "    })\n",
    "    # Here you would typically call the LLM with the response\n",
    "    # For demonstration, we'll just return the formatted messages\n",
    "\n",
    "    response = llm.invoke()\n",
    "    \n",
    "    return {\"response\": response}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend_api (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
