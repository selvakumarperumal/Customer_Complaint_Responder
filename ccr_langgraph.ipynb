{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "644de2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded: Yes\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"./.env\")\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "print(f\"API Key loaded: {'Yes' if GEMINI_API_KEY else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74157f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the LLM with proper API key handling\n",
    "try:\n",
    "    llm = ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-flash\",  # Using correct model name\n",
    "        temperature=0.1,\n",
    "        google_api_key=GEMINI_API_KEY\n",
    "    )\n",
    "    print(\"LLM initialized successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing LLM: {e}\")\n",
    "    print(\"Please check your GEMINI_API_KEY in the .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e398e423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c897ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that classifies customer complaints.\"),\n",
    "    (\"user\", \"complaint category: delivery, refund, product issue, other.\"),\n",
    "    (\"user\", \"classify the following complaint: {input}\")\n",
    "\n",
    "])\n",
    "\n",
    "response_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that generates appropriate response.\"),\n",
    "    (\"user\", \"Generate a professional and empathetic response to the following complaint: {complaint}. The complaint type is {complaint_type}.\"),\n",
    "    (\"user\", \"Note : Only single response is allowed.\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be00c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_chain = category_prompt | llm\n",
    "response_chain = response_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dc52852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "\n",
    "parser = StrOutputParser()# Add output parser for cleaner responsesfrom pydantic import BaseModel, Field\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89daa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplaintState(BaseModel):\n",
    "    complaint: str = Field(..., description=\"The customer's complaint text\")\n",
    "    complaint_type: str = Field(default=\"\", description=\"The type of complaint\")\n",
    "    response: str = Field(default=\"\", description=\"The response to the complaint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6224bca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_classify(state: ComplaintState) -> ComplaintState:\n",
    "    # Get the AI response and extract content\n",
    "    ai_response = classify_chain.invoke({\"input\": state.complaint})\n",
    "    state.complaint_type = ai_response.content.strip().lower()\n",
    "    return state\n",
    "\n",
    "def node_respond(state: ComplaintState) -> ComplaintState:\n",
    "    # Get the AI response and extract content\n",
    "    ai_response = response_chain.invoke({\n",
    "        \"complaint\": state.complaint,\n",
    "        \"complaint_type\": state.complaint_type\n",
    "    })\n",
    "    state.response = ai_response.content\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051a607e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x799d3022eae0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(ComplaintState)\n",
    "workflow.add_node(\"classify\", node_classify)\n",
    "workflow.add_node(\"respond\", node_respond)\n",
    "\n",
    "workflow.set_entry_point(\"classify\")\n",
    "workflow.add_edge(\"classify\", \"respond\")\n",
    "workflow.add_edge(\"respond\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e50688b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "app = workflow.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fd382ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complaint Type: delivery\n",
      "Response: Dear Valued Customer,\n",
      "\n",
      "We sincerely apologize that your order (#12345) has not yet arrived and is past its delivery date. We understand this is frustrating, and we want to assure you that we're taking this seriously.  We're currently investigating the delay and will provide you with an update within [ timeframe, e.g., 24-48 hours].  In the meantime, please could you provide us with your tracking number so we can expedite the process?  We appreciate your patience and understanding.\n"
     ]
    }
   ],
   "source": [
    "# Fix the configuration for checkpointer\n",
    "config = {\"configurable\": {\"thread_id\": \"complaint_thread_1\"}}\n",
    "\n",
    "state = {\"complaint\": \"My order #12345 hasn't arrived yet and it's past the delivery date.\"}\n",
    "result = app.invoke(state, config=config)\n",
    "print(f\"Complaint Type: {result['complaint_type']}\")\n",
    "print(f\"Response: {result['response']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28f18178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Multiple Complaints (Clean) ===\n",
      "\n",
      "0. Testing: I want to return my product, it doesn't work properly\n",
      "   Type: product issue and refund\n",
      "   Response: We're so sorry to hear you're experiencing trouble with your product and that it's not functioning as expected.  We understand this is frustrating, an...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Testing: Where is my refund? It's been 2 weeks since I returned the item\n",
      "   Type: refund\n",
      "   Response: I understand your frustration regarding your refund; it's certainly not ideal to wait longer than expected.  We sincerely apologize for the delay.  Ou...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "2. Testing: The delivery guy was rude to me yesterday\n",
      "   Type: delivery\n",
      "   Response: We sincerely apologize that you had a negative experience with our delivery driver yesterday.  We value your business and are disappointed to hear abo...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "3. Testing: Your website keeps crashing when I try to make a payment\n",
      "   Type: product issue (website functionality is a product issue).\n",
      "   Response: I'm so sorry to hear you're experiencing trouble making a payment on our website.  Website crashes are incredibly frustrating, and I understand your c...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Create a function to process complaints without memory persistence\n",
    "def process_single_complaint(complaint_text: str, thread_id: str = None):\n",
    "    \"\"\"Process a single complaint with fresh state\"\"\"\n",
    "    if thread_id is None:\n",
    "        thread_id = f\"thread_{hash(complaint_text) % 10000}\"\n",
    "    \n",
    "    config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "    state = {\"complaint\": complaint_text}\n",
    "    \n",
    "    try:\n",
    "        result = app.invoke(state, config=config)\n",
    "        return {\n",
    "            \"complaint\": complaint_text,\n",
    "            \"type\": result['complaint_type'],\n",
    "            \"response\": result['response']\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"complaint\": complaint_text,\n",
    "            \"type\": \"error\",\n",
    "            \"response\": f\"Error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Test with multiple complaints using unique thread IDs\n",
    "test_complaints = [\n",
    "    \"I want to return my product, it doesn't work properly\",\n",
    "    \"Where is my refund? It's been 2 weeks since I returned the item\", \n",
    "    \"The delivery guy was rude to me yesterday\",\n",
    "    \"Your website keeps crashing when I try to make a payment\"\n",
    "]\n",
    "\n",
    "print(\"=== Testing Multiple Complaints (Clean) ===\")\n",
    "for i, complaint in enumerate(test_complaints):\n",
    "    print(f\"\\n{i}. Testing: {complaint}\")\n",
    "    \n",
    "    # Use unique thread ID for each complaint to avoid memory interference\n",
    "    result = process_single_complaint(complaint, f\"unique_thread_{i}_{hash(complaint)}\")\n",
    "    \n",
    "    print(f\"   Type: {result['type']}\")\n",
    "    print(f\"   Response: {result['response'][:150]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "backend_api (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
